{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spark instance\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "        .master('local')\\\n",
    "        .appName('stopwrds') \\\n",
    "    .config('spark.executor.memory', '5gb') \\\n",
    "    .config(\"spark.cores.max\", \"6\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------+\n",
      "|id |raw                         |\n",
      "+---+----------------------------+\n",
      "|0  |[I, saw, the, red, balloon] |\n",
      "|1  |[Mary, had, a, little, lamb]|\n",
      "+---+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an example\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0, [\"I\", \"saw\", \"the\", \"red\", \"balloon\"]),\n",
    "    (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"]),\n",
    "], [\"id\", \"raw\"])\n",
    "\n",
    "sentenceData.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- raw: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- filtered: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------------------+--------------------+\n",
      "|id |raw                         |filtered            |\n",
      "+---+----------------------------+--------------------+\n",
      "|0  |[I, saw, the, red, balloon] |[saw, red, balloon] |\n",
      "|1  |[Mary, had, a, little, lamb]|[Mary, little, lamb]|\n",
      "+---+----------------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")\n",
    "filteredData = remover.transform(sentenceData)\n",
    "\n",
    "display(type(filteredData))\n",
    "display(filteredData.printSchema())\n",
    "filteredData.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class StopWordsRemover in module pyspark.ml.feature:\n",
      "\n",
      "class StopWordsRemover(pyspark.ml.wrapper.JavaTransformer, pyspark.ml.param.shared.HasInputCol, pyspark.ml.param.shared.HasOutputCol, pyspark.ml.util.JavaMLReadable, pyspark.ml.util.JavaMLWritable)\n",
      " |  A feature transformer that filters out stop words from input.\n",
      " |  \n",
      " |  .. note:: null values from input array are preserved unless adding null to stopWords explicitly.\n",
      " |  \n",
      " |  >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],)], [\"text\"])\n",
      " |  >>> remover = StopWordsRemover(inputCol=\"text\", outputCol=\"words\", stopWords=[\"b\"])\n",
      " |  >>> remover.transform(df).head().words == ['a', 'c']\n",
      " |  True\n",
      " |  >>> stopWordsRemoverPath = temp_path + \"/stopwords-remover\"\n",
      " |  >>> remover.save(stopWordsRemoverPath)\n",
      " |  >>> loadedRemover = StopWordsRemover.load(stopWordsRemoverPath)\n",
      " |  >>> loadedRemover.getStopWords() == remover.getStopWords()\n",
      " |  True\n",
      " |  >>> loadedRemover.getCaseSensitive() == remover.getCaseSensitive()\n",
      " |  True\n",
      " |  \n",
      " |  .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      StopWordsRemover\n",
      " |      pyspark.ml.wrapper.JavaTransformer\n",
      " |      pyspark.ml.wrapper.JavaParams\n",
      " |      pyspark.ml.wrapper.JavaWrapper\n",
      " |      pyspark.ml.base.Transformer\n",
      " |      pyspark.ml.param.shared.HasInputCol\n",
      " |      pyspark.ml.param.shared.HasOutputCol\n",
      " |      pyspark.ml.param.Params\n",
      " |      pyspark.ml.util.Identifiable\n",
      " |      pyspark.ml.util.JavaMLReadable\n",
      " |      pyspark.ml.util.MLReadable\n",
      " |      pyspark.ml.util.JavaMLWritable\n",
      " |      pyspark.ml.util.MLWritable\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, inputCol=None, outputCol=None, stopWords=None, caseSensitive=False)\n",
      " |      __init__(self, inputCol=None, outputCol=None, stopWords=None, caseSensitive=false)\n",
      " |  \n",
      " |  getCaseSensitive(self)\n",
      " |      Gets the value of :py:attr:`caseSensitive` or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  getStopWords(self)\n",
      " |      Gets the value of :py:attr:`stopWords` or its default value.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  setCaseSensitive(self, value)\n",
      " |      Sets the value of :py:attr:`caseSensitive`.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  setParams(self, inputCol=None, outputCol=None, stopWords=None, caseSensitive=False)\n",
      " |      setParams(self, inputCol=None, outputCol=None, stopWords=None, caseSensitive=false)\n",
      " |      Sets params for this StopWordRemover.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  setStopWords(self, value)\n",
      " |      Sets the value of :py:attr:`stopWords`.\n",
      " |      \n",
      " |      .. versionadded:: 1.6.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  loadDefaultStopWords(language)\n",
      " |      Loads the default stop words for the given language.\n",
      " |      Supported languages: danish, dutch, english, finnish, french, german, hungarian,\n",
      " |      italian, norwegian, portuguese, russian, spanish, swedish, turkish\n",
      " |      \n",
      " |      .. versionadded:: 2.0.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  caseSensitive = Param(parent='undefined', name='caseSensitive', ...a c...\n",
      " |  \n",
      " |  stopWords = Param(parent='undefined', name='stopWords', doc='The words...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.wrapper.JavaTransformer:\n",
      " |  \n",
      " |  __metaclass__ = <class 'abc.ABCMeta'>\n",
      " |      Metaclass for defining Abstract Base Classes (ABCs).\n",
      " |      \n",
      " |      Use this metaclass to create an ABC.  An ABC can be subclassed\n",
      " |      directly, and then acts as a mix-in class.  You can also register\n",
      " |      unrelated concrete classes (even built-in classes) and unrelated\n",
      " |      ABCs as 'virtual subclasses' -- these and their descendants will\n",
      " |      be considered subclasses of the registering ABC by the built-in\n",
      " |      issubclass() function, but the registering ABC won't show up in\n",
      " |      their MRO (Method Resolution Order) nor will method\n",
      " |      implementations defined by the registering ABC be callable (not\n",
      " |      even via super()).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.wrapper.JavaParams:\n",
      " |  \n",
      " |  __del__(self)\n",
      " |  \n",
      " |  copy(self, extra=None)\n",
      " |      Creates a copy of this instance with the same uid and some\n",
      " |      extra params. This implementation first calls Params.copy and\n",
      " |      then make a copy of the companion Java pipeline component with\n",
      " |      extra params. So both the Python wrapper and the Java pipeline\n",
      " |      component get copied.\n",
      " |      \n",
      " |      :param extra: Extra parameters to copy to the new instance\n",
      " |      :return: Copy of this instance\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.ml.wrapper.JavaWrapper:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.base.Transformer:\n",
      " |  \n",
      " |  transform(self, dataset, params=None)\n",
      " |      Transforms the input dataset with optional parameters.\n",
      " |      \n",
      " |      :param dataset: input dataset, which is an instance of :py:class:`pyspark.sql.DataFrame`\n",
      " |      :param params: an optional param map that overrides embedded params.\n",
      " |      :returns: transformed dataset\n",
      " |      \n",
      " |      .. versionadded:: 1.3.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasInputCol:\n",
      " |  \n",
      " |  getInputCol(self)\n",
      " |      Gets the value of inputCol or its default value.\n",
      " |  \n",
      " |  setInputCol(self, value)\n",
      " |      Sets the value of :py:attr:`inputCol`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasInputCol:\n",
      " |  \n",
      " |  inputCol = Param(parent='undefined', name='inputCol', doc='input colum...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.shared.HasOutputCol:\n",
      " |  \n",
      " |  getOutputCol(self)\n",
      " |      Gets the value of outputCol or its default value.\n",
      " |  \n",
      " |  setOutputCol(self, value)\n",
      " |      Sets the value of :py:attr:`outputCol`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pyspark.ml.param.shared.HasOutputCol:\n",
      " |  \n",
      " |  outputCol = Param(parent='undefined', name='outputCol', doc='output co...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  explainParam(self, param)\n",
      " |      Explains a single param and returns its name, doc, and optional\n",
      " |      default value and user-supplied value in a string.\n",
      " |  \n",
      " |  explainParams(self)\n",
      " |      Returns the documentation of all params with their optionally\n",
      " |      default values and user-supplied values.\n",
      " |  \n",
      " |  extractParamMap(self, extra=None)\n",
      " |      Extracts the embedded default param values and user-supplied\n",
      " |      values, and then merges them with extra values from input into\n",
      " |      a flat param map, where the latter value is used if there exist\n",
      " |      conflicts, i.e., with ordering: default param values <\n",
      " |      user-supplied values < extra.\n",
      " |      \n",
      " |      :param extra: extra param values\n",
      " |      :return: merged param map\n",
      " |  \n",
      " |  getOrDefault(self, param)\n",
      " |      Gets the value of a param in the user-supplied param map or its\n",
      " |      default value. Raises an error if neither is set.\n",
      " |  \n",
      " |  getParam(self, paramName)\n",
      " |      Gets a param by its name.\n",
      " |  \n",
      " |  hasDefault(self, param)\n",
      " |      Checks whether a param has a default value.\n",
      " |  \n",
      " |  hasParam(self, paramName)\n",
      " |      Tests whether this instance contains a param with a given\n",
      " |      (string) name.\n",
      " |  \n",
      " |  isDefined(self, param)\n",
      " |      Checks whether a param is explicitly set by user or has\n",
      " |      a default value.\n",
      " |  \n",
      " |  isSet(self, param)\n",
      " |      Checks whether a param is explicitly set by user.\n",
      " |  \n",
      " |  set(self, param, value)\n",
      " |      Sets a parameter in the embedded param map.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyspark.ml.param.Params:\n",
      " |  \n",
      " |  params\n",
      " |      Returns all params ordered by name. The default implementation\n",
      " |      uses :py:func:`dir` to get all attributes of type\n",
      " |      :py:class:`Param`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.Identifiable:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.JavaMLReadable:\n",
      " |  \n",
      " |  read() from builtins.type\n",
      " |      Returns an MLReader instance for this class.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pyspark.ml.util.MLReadable:\n",
      " |  \n",
      " |  load(path) from builtins.type\n",
      " |      Reads an ML instance from the input path, a shortcut of `read().load(path)`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.JavaMLWritable:\n",
      " |  \n",
      " |  write(self)\n",
      " |      Returns an MLWriter instance for this ML instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pyspark.ml.util.MLWritable:\n",
      " |  \n",
      " |  save(self, path)\n",
      " |      Save this ML instance to the given path, a shortcut of 'write().save(path)'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(StopWordsRemover)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
