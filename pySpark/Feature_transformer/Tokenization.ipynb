{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a spark instance\n",
    "from pyspark.sql import SparkSession\n",
    "# initialise sparkContext\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local') \\\n",
    "    .appName('muthootSample1') \\\n",
    "    .config('spark.executor.memory', '5gb') \\\n",
    "    .config(\"spark.cores.max\", \"6\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            sentence|\n",
      "+---+--------------------+\n",
      "|  0|Hi I heard about ...|\n",
      "|  1|I wish Java could...|\n",
      "|  2|Logistic regressi...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## example\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "sentenceDataFrame = spark.createDataFrame([\n",
    "    (0, \"Hi I heard about Spark\"),\n",
    "    (1, \"I wish Java could use case classes\"),\n",
    "    (2, \"Logistic regression, models, are, neat\")\n",
    "], [\"id\", \"sentence\"])\n",
    "sentenceDataFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- sentence: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------+--------------------------------------------+\n",
      "|id |sentence                              |words                                       |\n",
      "+---+--------------------------------------+--------------------------------------------+\n",
      "|0  |Hi I heard about Spark                |[hi, i, heard, about, spark]                |\n",
      "|1  |I wish Java could use case classes    |[i, wish, java, could, use, case, classes]  |\n",
      "|2  |Logistic regression, models, are, neat|[logistic, regression,, models,, are,, neat]|\n",
      "+---+--------------------------------------+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "# Tokenize the data\n",
    "tokenized = tokenizer.transform(sentenceDataFrame)\n",
    "\n",
    "display(type(tokenized))\n",
    "display(tokenized.printSchema())\n",
    "tokenized.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "- 結果是dataframe\n",
    "    - 存在outputCol 中為array\n",
    "- 自動將大寫轉成小寫\n",
    "- 依據whitespace做切割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a user defined function for counting\n",
    "## lambda function + datatype\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------+--------------------------------------------+------+\n",
      "|id |sentence                              |words                                       |tokens|\n",
      "+---+--------------------------------------+--------------------------------------------+------+\n",
      "|0  |Hi I heard about Spark                |[hi, i, heard, about, spark]                |5     |\n",
      "|1  |I wish Java could use case classes    |[i, wish, java, could, use, case, classes]  |7     |\n",
      "|2  |Logistic regression, models, are, neat|[logistic, regression,, models,, are,, neat]|5     |\n",
      "+---+--------------------------------------+--------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized.withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization with Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- sentence: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------+------------------------------------------+\n",
      "|id |sentence                              |words                                     |\n",
      "+---+--------------------------------------+------------------------------------------+\n",
      "|0  |Hi I heard about Spark                |[hi, i, heard, about, spark]              |\n",
      "|1  |I wish Java could use case classes    |[i, wish, java, could, use, case, classes]|\n",
      "|2  |Logistic regression, models, are, neat|[logistic, regression, models, are, neat] |\n",
      "+---+--------------------------------------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of regexTokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")  ### \\\\W = [^a-zA-Z0-9_], i.e., 找尋非文字數字當作切割\n",
    "# Tokenize the data with the pattern\n",
    "regexTokenized = regexTokenizer.transform(sentenceDataFrame)\n",
    "\n",
    "display(type(regexTokenized))\n",
    "display(regexTokenized.printSchema())\n",
    "regexTokenized.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------------------------+------------------------------------------+---------+\n",
      "|id |sentence                              |words                                     |regTokens|\n",
      "+---+--------------------------------------+------------------------------------------+---------+\n",
      "|0  |Hi I heard about Spark                |[hi, i, heard, about, spark]              |5        |\n",
      "|1  |I wish Java could use case classes    |[i, wish, java, could, use, case, classes]|7        |\n",
      "|2  |Logistic regression, models, are, neat|[logistic, regression, models, are, neat] |5        |\n",
      "+---+--------------------------------------+------------------------------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regexTokenized.withColumn(\"regTokens\", countTokens(col(\"words\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "- 結果是dataframe\n",
    "    - 存在outputCol 中為array\n",
    "- 自動將大寫轉成小寫\n",
    "- 依據regex pattern做切割"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
