{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note 2: optimization for a neural network\n",
    "WT Wang                     \n",
    "May, 03, 2017\n",
    "\n",
    "------\n",
    "\n",
    "## Loss function\n",
    "After fitting the model based on the certain weights, we can also calculate the prediction errors to see how accurate the predictions are to the actual value. The prediction error is , $e_i({\\bf{w}}) = y_i - \\hat{y}_i({\\bf{w}})$. However, how can we know the current predictions are the best ones? We can measure these by a loss function . For example, a squared error loss function is $$L_1({\\bf{w}}) = \\sum_{i=1}^n e^2_i({\\bf{w}});$$ an absolute error loss functionis $$L_2({\\bf{w}}) = \\sum_{i=1}^n \\left|e_i({\\bf{w}})\\right|.$$ That is, a lower loss function value means a better model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice: toy example\n",
    "\n",
    "Set two candidate sets of the weights as \n",
    "    - 1st set: \n",
    "    \n",
    "${\\bf{\\omega}}_{11} = \\left[ \\begin{array}{cc}\n",
    "-5 & 5 \\\\\n",
    "-1 & 1\\\\\n",
    "\\end{array} \\right]$, ${\\bf{\\gamma}}_1 = \\left[ \\begin{array}{c}\n",
    "3  \\\\\n",
    "7\\\\\n",
    "\\end{array} \\right]$\n",
    "     - 2nd set: \n",
    "    \n",
    "${\\bf{\\omega}}_{12} = \\left[ \\begin{array}{cc}\n",
    "1 & 1 \\\\\n",
    "2 & -1.5\\\\\n",
    "\\end{array} \\right]$, ${\\bf{\\gamma}}_2 = \\left[ \\begin{array}{c}\n",
    "1.9  \\\\\n",
    "3.5\\\\\n",
    "\\end{array} \\right]$.\n",
    "\n",
    "\n",
    "The input dataset is ${\\bf{X}}= \\left[ \\begin{array}{cc}\n",
    "0 & 1\\\\\n",
    "5 & 7\\\\\n",
    "6 & -2\\\\\n",
    "10 & 11\\\\\n",
    "\\end{array} \\right]$, and the response is  ${\\bf{y}}= \\left[ \\begin{array}{c}\n",
    "7\\\\\n",
    "5\\\\\n",
    "2\\\\\n",
    "20\\\\\n",
    "\\end{array} \\right]$. Use `predict_with_one_layer()` to calculate the predicitons, and the squared error loss function to measure the performance. The result below shows that the second candidate set of weights perfoms better than the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error with weights_1: 1779.500000\n",
      "Mean squared error with weights_2: 1188.020625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "# import the functions in note1\n",
    "from note1 import *\n",
    "\n",
    "# Create model_output_1 \n",
    "model_output_1 = []\n",
    "# Create model_output_2\n",
    "model_output_2 = []\n",
    "\n",
    "weights_1 = {'node_0': np.array([-5, 5]),\n",
    "            'node_1': np.array([-1, 1]),\n",
    "            'output': np.array([7, 5])}\n",
    "\n",
    "weights_2 = {'node_0': np.array([1, 1]),\n",
    "            'node_1': np.array([2, -1.5]),\n",
    "            'output': np.array([1.9, 3.5])}\n",
    "\n",
    "input_data = [np.array([0, 1]),\n",
    "              np.array([5, 7]),\n",
    "              np.array([6, -2]),\n",
    "              np.array([10, 11])]\n",
    "\n",
    "target_actuals = [7, 5, 2, 20]\n",
    "\n",
    "# Loop over input_data\n",
    "for row in input_data:\n",
    "    # Append prediction to model_output_0\n",
    "    model_output_1.append(predict_with_one_layer(row, weights_1))\n",
    "    \n",
    "    # Append prediction to model_output_1\n",
    "    model_output_2.append(predict_with_one_layer(row, weights_2))\n",
    "\n",
    "# Calculate the mean squared error for model_output_0: mse_0\n",
    "mse_1 = mean_squared_error(model_output_1, target_actuals)\n",
    "\n",
    "# Calculate the mean squared error for model_output_1: mse_1\n",
    "mse_2 = mean_squared_error(model_output_2, target_actuals)\n",
    "\n",
    "# Print mse_0 and mse_1\n",
    "print(\"Mean squared error with weights_1: %f\" %mse_1)\n",
    "print(\"Mean squared error with weights_2: %f\" %mse_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: we can see that the sencond weight performs better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "Here, we want to find the weights that give the lowest value for the loss function.\n",
    "\n",
    "### Gradient decent (GD)\n",
    "Simply, we can apply the gradient descent to address this problem. Gradient descent is a first-order iterative optimization algorithm, that is, the solution is computed along with the paths of the slope of loss function with respect to the weights. More detail can be found in [here](https://en.wikipedia.org/wiki/Gradient_descent). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def gradientDescent(input_data, target, weights, learning_rate = 0.01, n_updates = 5):\n",
    "    \n",
    "    # Iterate over the number of updates\n",
    "    for i in range(n_updates):\n",
    "        # Calculate the predictions: preds\n",
    "        preds = (weights * input_data).sum()\n",
    "        # Calculate the error: error\n",
    "        error = target - preds\n",
    "        # Calculate the slope: slope\n",
    "        slope = 2 * input_data * error\n",
    "        # Update the weights: weights\n",
    "        weights = weights + slope * learning_rate\n",
    "\n",
    "        mse = (error) **2\n",
    "          \n",
    "        print(\"Iteration %d -- loss: %f\" % (i+1, mse))\n",
    "     \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice: toy example\n",
    "\n",
    "Set a initial vector of weights as   \n",
    "${\\bf{\\omega}} = \\left[ \\begin{array}{c}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{array} \\right]$, the input dataset is ${\\bf{X}}= \\left[ \\begin{array}{c}\n",
    "3\\\\\n",
    "1\\\\\n",
    "5\n",
    "\\end{array} \\right]$, and the response is  $ y=8$. Use `gradientDescent()` to calculate the estimation of weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 -- loss: 64.000000\n",
      "Iteration 2 -- loss: 5.760000\n",
      "Iteration 3 -- loss: 0.518400\n",
      "Iteration 4 -- loss: 0.046656\n",
      "Iteration 5 -- loss: 0.004199\n",
      "[-0.684048 -0.228016 -1.14008 ]\n"
     ]
    }
   ],
   "source": [
    "weights = np.array([0, 0, 0])     \n",
    "input_data = np.array([3, 1, 5])\n",
    "target = -8\n",
    "new_weight = gradientDescent(input_data, target, weights)\n",
    "print(new_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Note: we can see that the loss decreases as the number of iteration gets larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent (SGD)\n",
    "- The process of SGD is:\n",
    "     1. It is common to calculate slopes on only a subset of the randomly shuffled data (‘batch’)\n",
    "     2. Use a different batch of data to calculate the next update\n",
    "     3. Start over from the beginning once all data is used\n",
    "\n",
    "- The algorithm:\n",
    "       1. Randomly shuffle the data\n",
    "       2. Split m subsets\n",
    "       3. Do{\n",
    "             for i = 1, ..., m:\n",
    "                 \\omega_i := \\omega_i - learning rate * slope\n",
    "          } until converge\n",
    "\n",
    "Remark: SGD usually converges faster than GD with a mild convergence rate. More detail can be found [here](http://cs229.stanford.edu/notes/cs229-notes1.pdf).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "The process of backpropagation is:\n",
    "1. Start at some random set of weights\n",
    "2. Use forward propagation to make a prediction\n",
    "3. Use backward propagation to calculate the slope of the loss function w.r.t each weight\n",
    "4. Multiply that slope by the learning rate, and subtract from the current weights\n",
    "5. Keep going with that cycle until we get to a flat part\n",
    "\n",
    "Remark: More detail can be found [here](https://page.mi.fu-berlin.de/rojas/neural/chapter/K7.pdf).  \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
